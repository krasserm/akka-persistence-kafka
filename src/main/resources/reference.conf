akka {
  actor {
    serializers {
      kafka-event = "akka.persistence.kafka.journal.KafkaEventSerializer"
      kafka-snapshot = "akka.persistence.kafka.snapshot.KafkaSnapshotSerializer"
    }

    serialization-bindings {
      "akka.persistence.kafka.Event" = kafka-event
      "akka.persistence.kafka.snapshot.KafkaSnapshot" = kafka-snapshot
    }
  }
}

kafka-journal {

  # FQCN of the Kafka journal plugin
  class = "akka.persistence.kafka.journal.KafkaJournal"

  # Dispatcher for the plugin actor
  plugin-dispatcher = "kafka-journal.default-dispatcher"

  # Number of concurrent writers (should be <= number of available threads in
  # dispatcher).
  write-concurrency = 8

  # The partition to use when publishing to and consuming from journal topics.
  partition = 0

  # Default dispatcher for plugin actor.
  default-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 8
    }
  }

  consumer {
    # -------------------------------------------------------------------
    # Simple consumer configuration (used for message replay and reading
    # metadata).
    #
    # See http://kafka.apache.org/documentation.html#consumerconfigs
    # See http://kafka.apache.org/documentation.html#simpleconsumerapi
    # -------------------------------------------------------------------

    socket.timeout.ms = 30000

    socket.receive.buffer.bytes = 65536

    fetch.message.max.bytes = 1048576
  }

  producer {
    # -------------------------------------------------------------------
    # PersistentRepr producer (to journal topics) configuration.
    #
    # See http://kafka.apache.org/documentation.html#producerconfigs
    #
    # The metadata.broker.list property is set dynamically by the journal.
    # No need to set it here.
    # -------------------------------------------------------------------

    # DO NOT CHANGE!
    partitioner.class = "akka.persistence.kafka.StickyPartitioner"

    # DO NOT CHANGE!
    key.serializer.class = "kafka.serializer.StringEncoder"

    # Add further Kafka producer settings here, if needed.
    # ...
  }

  event.producer {
    # -------------------------------------------------------------------
    # Event producer (to user-defined topics) configuration.
    #
    # See http://kafka.apache.org/documentation.html#producerconfigs
    # -------------------------------------------------------------------

    request.required.acks = 0

    topic.mapper.class = "akka.persistence.kafka.DefaultEventTopicMapper"

    key.serializer.class = "kafka.serializer.StringEncoder"

    # Add further Kafka producer settings here, if needed.
    # ...
  }

  zookeeper {
    # -------------------------------------------------------------------
    # Zookeeper client configuration
    # -------------------------------------------------------------------

    connect = "localhost:2181"

    session.timeout.ms = 6000

    connection.timeout.ms = 6000

    sync.time.ms = 2000
  }
}

kafka-snapshot-store {

  # FQCN of the Kafka snapshot store plugin
  class = "akka.persistence.kafka.snapshot.KafkaSnapshotStore"

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "kafka-snapshot-store.default-dispatcher"

  # The partition to use when publishing to and consuming from snapshot topics.
  partition = 0

  # Topic name prefix (which prepended to persistenceId)
  prefix = "snapshot-"

  # If set to true snapshots with sequence numbers higher than the sequence number
  # of the latest entry in their corresponding journal topic are ignored. This is
  # necessary to recover from certain Kafka failure scenarios. Should only be set
  # to false for isolated snapshot store tests.
  ignore-orphan = true

  # Default dispatcher for plugin actor.
  default-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 8
    }
  }

  consumer {
    # -------------------------------------------------------------------
    # Simple consumer configuration (used for loading snapshots and
    # reading metadata).
    #
    # See http://kafka.apache.org/documentation.html#consumerconfigs
    # See http://kafka.apache.org/documentation.html#simpleconsumerapi
    # -------------------------------------------------------------------

    socket.timeout.ms = 30000

    socket.receive.buffer.bytes = 65536

    fetch.message.max.bytes = 1048576
  }

  producer {
    # -------------------------------------------------------------------
    # Snapshot producer configuration.
    #
    # See http://kafka.apache.org/documentation.html#producerconfigs
    #
    # The metadata.broker.list property is set dynamically by the journal.
    # No need to set it here.
    # -------------------------------------------------------------------

    request.required.acks = 1

    producer.type = "sync"

    # DO NOT CHANGE!
    partitioner.class = "akka.persistence.kafka.StickyPartitioner"

    # DO NOT CHANGE!
    key.serializer.class = "kafka.serializer.StringEncoder"

    # Increase if hundreds of topics are created during initialization.
    message.send.max.retries = 5

    # Increase if hundreds of topics are created during initialization.
    retry.backoff.ms = 500

    # Add further Kafka producer settings here, if needed.
    # ...
  }

  zookeeper {
    # -------------------------------------------------------------------
    # Zookeeper client configuration
    # -------------------------------------------------------------------

    connect = "localhost:2181"

    session.timeout.ms = 6000

    connection.timeout.ms = 6000

    sync.time.ms = 2000
  }
}

test-server {
  # -------------------------------------------------------------------
  # Test Kafka and Zookeeper server configuration.
  #
  # See http://kafka.apache.org/documentation.html#brokerconfigs
  # -------------------------------------------------------------------

  zookeeper {

    port = 2181

    dir = "data/zookeeper"
  }

  kafka {

    broker.id = 1

    port = 6667

    num.partitions = 2

    log.cleanup.policy = "compact"

    log.dirs = data/kafka

    log.index.size.max.bytes = 1024

    transaction.state.log.replication.factor = 1
    transaction.state.log.min.isr = 1
    offsets.topic.replication.factor = 1

    min.insync.replicas = 1
  }
}